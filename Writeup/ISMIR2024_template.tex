% -----------------------------------------------
% Template for ISMIR Papers
% 2023 version, based on previous ISMIR templates

% Requirements :
% * 6+n page length maximum
% * 10MB maximum file size
% * Copyright note must appear in the bottom left corner of first page
% * Clearer statement about citing own work in anonymized submission
% (see conference website for additional details)
% -----------------------------------------------

\documentclass{article}
\usepackage[T1]{fontenc} % add special characters (e.g., umlaute)
\usepackage[utf8]{inputenc} % set utf-8 as default input encoding
\usepackage{ismir,amsmath,cite,url}
\usepackage{graphicx}
\usepackage{color}
\usepackage{amsfonts}


\usepackage{lineno}
\linenumbers

\graphicspath{{../Figures/}}

% Title. Please use IEEE-compliant title case when specifying the title here,
% as it has implications for the copyright notice
% ------
\title{The Concatenator: A Bayesian Approach To Real Time Concatenative Musaicing}

% Note: Please do NOT use \thanks or a \footnote in any of the author markup

% Single address
% To use with only one author or several with the same address
% ---------------
%\oneauthor
% {Names should be omitted for double-blind reviewing}
% {Affiliations should be omitted for double-blind reviewing}

% Two addresses
% --------------
\twoauthors
  {First author} {School \\ Department}
  {Second author} {Company \\ Address}

% Three addresses
% --------------\input{ISMIR2021_paper.tex}

%\threeauthors
%  {First Author} {Affiliation1 \\ {\tt author1@ismir.edu}}
%  {Second Author} {\bf Retain these fake authors in\\\bf submission to preserve the formatting}
%  {Third Author} {Affiliation3 \\ {\tt author3@ismir.edu}}

% Four or more addresses
% OR alternative format for large number of co-authors
% ------------
%\multauthor
%{First author$^1$ \hspace{1cm} Second author$^1$ \hspace{1cm} Third author$^2$} { \bfseries{Fourth author$^3$ \hspace{1cm} Fifth author$^2$ \hspace{1cm} Sixth author$^1$}\\
%  $^1$ Department of Computer Science, University , Country\\
%$^2$ International Laboratories, City, Country\\
%$^3$  Company, Address\\
%{\tt\small CorrespondenceAuthor@ismir.edu, PossibleOtherAuthor@ismir.edu}
%}

% For the author list in the Creative Common license, please enter author names. 
% Please abbreviate the first names of authors and add 'and' between the second to last and last authors.
\def\authorname{F. Author, S. Author}

% Optional: To use hyperref, uncomment the following.
\usepackage[bookmarks=false,pdfauthor={\authorname},pdfsubject={\papersubject},hidelinks]{hyperref}
% Mind the bookmarks=false option; bookmarks are incompatible with ismir.sty.

\sloppy % please retain sloppy command for improved formatting

\newcommand{\ChrisEdit}[1]{\textcolor{red}{(#1)}}
\newcommand{\BenEdit}[1]{\textcolor{blue}{(#1)}}

\begin{document}

%
\maketitle
%

\begin{abstract}
    We present ``The Concatenator,'' a real time system for audio-guided concatenative synthesis. Similarly to Driedger et. al's ``musaicing'' (or ``audio mosaicing'') technique, we concatenate a set number of activations within a corpus of audio to re-create the harmonic and percussive aspects of a target audio stream. Unlike Driedger's NMF-based technique, however, we instead use an explicitly Bayesian point of view, where corpus activations are hidden states and the target audio stream is an observation. We use a particle filter to infer the best hidden corpus states in real-time. Our transition model includes a tunable parameter to control the time-continuity of corpus grains, and our observation model allows users to prioritize how quickly the activations shift to match the target. Because the computational complexity of the system is independent of the corpus size, our system scales to corpora that are hours long, which is an important feature in the age of vast audio data collections. Within The Concatenator module itself, composers can vary grain length, fit to target, and pitch shift in real time while reacting to the sounds they hear, enabling them to rapidly iterate ideas. To conclude our work, we evaluate our system with extensive quantitative tests of the effects of parameters, as well as a qualitative evaluation with artistic insights. Based on the quality of the results, we believe the real-time capability unlocks new avenues for musical expression and control, suitable for live performance and modular synthesis integration, which furthermore represents an essential breakthrough in concatenative synthesis technology.
    
\end{abstract}

\section{Introduction}

Concatenative Synthesis, or Audio Mosaicing, is a data-driven approach towards arranging granular fragments of audio samples, particularly using data sourced from the spectral-temporal features of a target sound. While granular synthesis systems typically rely on combinations of aleatoric parameterization, deterministic automation, and traditional synthesis modulation to achieve complex and evolving textures from sound fragments, Concatenative Synthesis algorithms utilize Music Information Retrieval technology to decide parameters such as the index, amplitude, and pitch of each sound fragment. As data storage and processing capabilities expand each year, new methods for harnessing the power of MIR are becoming available for music makers, along with previously unimaginable new avenues for creative expression in sound design and composition. 

Modern music producers are inundated by audio data. Services like Splice offer hundreds of thousands of samples readily available on the cloud, and Kontakt multi-sample libraries can often take up over 10gb of disk space to capture a single instrument. Music Producers generate plenty of their own audio data as well: stems, multi-tracks, long-form recordings, and mix variations account for a large portion of many a music producer's audio collection. Recent software such as XO by XLN Audio, Sononym, and Ableton Live 12 offer automatic organization of audio files based on various tags and descriptors, but these implementations of MIR technology are more utilitarian than creative in their design and application. Meanwhile, concatenative synthesis options remain sparse since its conceptual inception \cite{schwarz2000system}: Reformer by Krotos is designed to create foley designs, apps like Samplebrain and CataRT are lacking in critical musical areas such as pitch detection, with the more advanced options having limited accessibility for artists, requiring prior knowledge of Max (FluCoMa, MuBu) or Python (Audioguide).

The Concatenator aims to represent a breakthrough in concatenative synthesis technology that satisfies three major areas of advancement: 1) The system is capable of accurate harmonic and percussive reproductions from arbitrary corpora 2) in real-time, 3) affording new levels of control and accessibility for users. The speed, ease, and scope at which The Concatenator should allow for the creation of audio mosaics will contribute to the ongoing research and innovation in this field. We also hope that this project offers a fresh paradigm for music producers to interact creatively with their ever-expanding excess of audio data. 

\section{Related Work}
This work builds on a long history of work in Bayesian inference and particle filters, concatenative synthesis, and applied nonnegative matrix factorization (NMF).

From an artistic point of view, the most similar technique to ours is Driedger et al.'s 2015 ``Let It Bee'' concatenative musaicing technique \cite{driedger2015let}, which uses NMF to learn activations of spectral templates so that their combination will match a target spectrogram. This technique proved to represent a fruitful innovation in sound design for electronic music production, being featured heavily on Zero Point by Rob Clouth (2020), using custom software also authored by Clouth. In 2020, the algorithm was implemented in an open source python script by Tralie (LetItBee 2020), and in Max by the FluCoMa project (fluid.bufnmfcross~ 2021), which made NMF-inspired audio mosaicing accessible enough to contribute towards the production of at least two more albums heavily featuring the technique: Edenic Mosaics by Encanti (2021) and Hate Devours Its Host by Valance Drakes (2023).


We now detail some of the mathematics of Driedger et al.'s technique, as we borrow a few ideas in our work.  Driedger et al. learn $H$ in the equation $V \approx WH$, where $V$ is an $M \times T$ target spectrogram with $M$ frequency bins and $T$ times, $W$ is an $M \times N$ set of $N$ spectral templates that are treated as fixed, and $H$ is a matrix of $N \times T$ learned activations.  For instance, $W$ could be the spectral windows of a collection of buzzing bees and $V$ could be an excerpt from The Beatles' ``Let It Be'' (hence the title).  Driedger et al. use the Kullback-Liebler (KL) divergence loss, an instance of the more general $\beta$-divergence \cite{buch2017nichtnegativematrixfaktorisierungnutzendesklangsynthesensystem}, to measure the goodness of fit of $WH$ to $V$.  This loss function is 

\begin{equation}
\label{eq:klloss}
D(V || WH) = \sum V \odot \log \left( \frac{V}{WH} \right) - V + WH
\end{equation}

where $\odot$, $/$, $+$, and $-$ are all applied element-wise, and the sum is taken over all elements of the resulting matrix.  As Lee/Seung show, choosing the right step size turns gradient descent of Equation~\ref{eq:klloss}, with respect to $W$ and $H$, into {\em multiplicative update rules} that guarantee monotonic convergence.  Driedger et al. keep $W$ fixed to force the final audio to use exact copies of the templates, so only the update rule for $H$ is relevant.  At iteration $\ell$, this is:

\begin{equation}
\label{eq:klhgrad}
H_{kt}^{\ell} \gets H_{kt}^{\ell-1} \left( \frac{ \sum_{n} W_{nk} V_{nt} / (WH^{\ell-1})_{nt} }{ \sum_{n} W_{nk} } \right)
\end{equation}

Crucially, though, Driedger et al. note that the update rules in Equation~\ref{eq:klhgrad} alone will lose the timbral character of the templates in $W$.  They hence disrupt ordinary KL gradient descent by performing several increasingly impactful modifications to $H$ before Equation~\ref{eq:klhgrad} in each step, which are eventually set in stone after $L$ total iterations.  First, they avoid repeated activations to avoid a ``jittering'' effect, allowing a particular activation $k$ to only exist once in some iterval based on where it's the strongest:
\begin{equation}
    \label{eq:driedgerrepeated}
    (H_r)_{kt}^{\ell} \gets \left\{ \begin{array}{cc} H^{\ell-1}_{kt} & H^{\ell-1}_{kt} > H^{\ell-1}_{ks}, |t - s| \leq r \\ H^{\ell-1}_{kt} (1 - \frac{\ell+1}{L}) & \text{otherwise}  \end{array} \right\}
\end{equation}
for some chosen $r$.  They also promote sparsity in a similar way by shrinking all but the top $p$ activations in each column of $H_r$ to create $H_p^{\ell}$.  Finally, they add a step to encourage {\em time continuous activations} by doing ``diagonal enhancement,'' or by doing a windowed sum down each diagonal of $H_p$, assuming the columns of $W$ are also in a time order\footnote{A multiscale music structure analysis work uses a similar idea \cite{mcfee2014analyzing}}:

\begin{equation}
    \label{eq:driedgertimecontinuous}
    (H_c)_{kt}^{\ell} = \sum_{i=-c}^c (H_p)^{\ell}_{k+i, t+i}
\end{equation}

Since this encourages the algorithm to mash up chunks of $W$ in a time order, it effectively encourages sound grains from the templates than the length of a single window that ordinary NMF would take.  Finally, Drieger at all apply Equation~\ref{eq:klhgrad} to $H_c^{\ell}$ instead of $H^{\ell-1}$ to obtain $H^{\ell}$.

Overall, these disruptions remove the guarantee that Equation~\ref{eq:klloss} will be minimized, or that it will even monotonically decrease, but Driedger et al.'s brilliant insight is that the loss function is merely a guide to choose reasonable activations; a sub-optimal fit leaves room to better preserve timbral characteristics.  We take a similar perspective.



\subsubsection{Beyond Driedger}
Interestingly, the idea of spectrogram decomposition used for concatenative musaicing goes back to the work of \cite{burred2013cross}.  Beyond that, the authors of \cite{buch2017nichtnegativematrixfaktorisierungnutzendesklangsynthesensystem} provide some improvements to Driedger et al.'s technique, including sidestepping phase retrieval, which we exploit in our work.

One issue with Driedger et al.'s technique is the sources have to be augmented with pitch shifts to span additional pitches in the target, increasing memory consumption and runtime.  The authors of \cite{foroughmand2017multi, aarabi2018music} sidestep this by using 2D deconvolutional NMF \cite{schmidt2006nonnegative}.  Specifically, their technique uses the Constant-Q transform, where pitch shifts are modeled as constant shifts of the activations instead of the templates, saving memory.  The other added activation dimension models time history and time shifts to avoid the need for the diagonal enhancement of Equation~\ref{eq:driedgertimecontinuous}.  Interestingly, the authors apply 2D NMF to both the source material and the target, so they do not preserve the original sound grains.  However, for our preferred style, we want to take the source grains exactly as they are.



Schwarz created an offline concatenative synthesis system dubbed ``Caterpillar'' that uses the Viterbi algorithm \cite{schwarz2000system}, which was later approximated with a real time system, "CataRT" that uses a greedy approach instead of the Viterbi algorithm \cite{schwarz2006real, schwarz2008principles}.  Simon's ``audio analogies'' is quite similar \cite{simon2005audio}, but instead of a user controlled traversal through timbral space, they use features from some source (e.g. midi audio) to guide synthesis to a target with a different timbre (e.g. real audio of someone playing a trumpet).  \BenEdit{Need some additional references on granular synthesis}.

Caterpillar and audio analogies are both {\em Bayesian in nature}, where the {\em hidden state} is the template to concatenate, and the ``observation'' is a user-controlled trajetory or features from a source timbre, respectively.  The prior transition probabilities are based on temporal continuity.  However, they use the Viterbi algorithm, which is computationally intensive and which needs all time history, so it cannot be applied in real time.  A particle filter is an alternative for real time Bayesian inference \ChrisEdit{find particle filter citations}.  Particle filters have found use in other real time MIR applications such as multi-pitch tracking \cite{duan2011state} and beat tracking \cite{heydari2021don}.  

\section{The Concatenator}

\subsection{Bayesian Formulation}
\label{sec:bayesian}

    The NMF technique of Driedger is not suitable for realtime applications, both because the gradient update rules of Equation~\ref{eq:klhgrad} are too slow, and because the equations to promote repeated activations and time-continuous at each entry of $H$ require knowledge of all activations in $H$, including future activations.  We instead turn to a Bayesian formulation, where we view the target spectrogram (or some derived feature) $V$ as the observation, where the $t^{\text{th}}$ column of $V$ is an observation at time $t$.  The hidden states are the activations of the $N$ templates in the corpus spectrogram $W$ at each time.  We then use a particle filter to efficiently infer the hidden activations (Section~\ref{sec:sampling}).
    
    Henceforth, we will refer to the observations, or columns of $V$ at time $t$ as vectors $\vec{v}_t$ to emphasize that the data is streaming, and we focus on one timestep $t$ at a time.  
    
    \textbf{State space.} To keep our state space simple, we decouple which activations are active from their weights; we model the former only as our hidden state, while we infer the weights as a best fit under the KL-loss (Equation~\ref{eq:klloss}).  To control for polyphany directly, we use a {\em $p$-sparse} integer-valued vector $\vec{s}_t \in \mathbb{Z}^p$ as our hidden state.  This vector indexes the $p$ templates that are active at time $t$, where $p$ is fixed ahead of time.  For convenience of implementation, we allow repetitions of template indices, and the indices are in no particular order\footnote{One could also model $\vec{s}_t$ as an unordered set without repetition, which would identify all $\vec{s}_t$'s that are permutations of each other.  This would reduce the cardinality of the state space to $\binom{N}{k}$ at the cost of a more cumbersome implementation.  We also rely on some redundancy of good state samples in our particle filter, so the redundancy of our simpler scheme does not hurt us.};

    \begin{equation} 
        \label{eq:statevector}
        \vec{s_t}[k] \in \left\{0, 1, ..., N-1\right\}, k = 0, 1, ..., p-1 
    \end{equation}

    In addition to being sparse, a lower $p$ also reduces the cardinality $N^p$ of the state space, which is helpful for inference, though there are some caveats given a ``lottery ticket'' advantage of randomly choosing more activations, as we explain in Section~\ref{sec:quantitativeevaluation}.  We then infer the associated nonnegative weights $\vec{h_t}[k]$ for each activation to give the approximation $\vec{\Lambda_t}$ at time $t$
    
    \begin{equation}
        \label{eq:approximation}
        \vec{\Lambda_t}[n] = \sum_{k} \vec{h_t}[k]  W_{n, \vec{s_t}[k]}
    \end{equation}

    In particular, given $W$, $\vec{s_t}$, and $\vec{v_t}$, we apply the update rules of Equation~\ref{eq:klhgrad} for a pre-specified number $L$ of iterations, using the corresponding columns $\vec{s_t}$ of $W$

    \begin{equation}
        \label{eq:sparseklhgrad}
        \vec{h_t}^{\ell}[k]  \gets \vec{h_t}^{\ell-1} \left(  \frac{\sum_n (W_{n, \vec{s_t}[k]}) (\vec{v_t}[n]) / (\vec{\Lambda_t}^{\ell-1}[n]) }{ \sum_{n} W_{n, \vec{s_t}[k]}} \right)
    \end{equation}
    

    \textbf{Transition Model.} We use the KL-loss (Equation~\ref{eq:klloss}) to measure the spectral fit of $\Lambda_t$ to $\vec{v_t}$, which we'll refer to as $d_t$.  As in Driedger et al. \cite{driedger2015let} (Equation~\ref{eq:driedgertimecontinuous}), however, we are willing to sacrifice fit to take longer grains from the corpus $W$.  To that end, we define the prior  \textbf{state transition probability} in the as a Factorial Hidden Markov Model (FHMM) \cite{ghahramani1995factorial}, where components of each activation transition independently from each other:

    \begin{equation}
        \label{eq:transitionprob}
        p_T(\vec{h_t} = \vec{b} | \vec{h_{t-1}} = \vec{a}) = \prod_{k=0}^{p-1} \left\{  \begin{array}{cc}  p_d & b[k] = a[k]+1  \\ \frac{1-p_d}{N-1} & \text{otherwise} \end{array} \right\}
    \end{equation}
    where $p_d \in [0, 1]$ is the ``probability of remaining time-continuous.''  Intuitively, if $p_d > 0.5$, then we are much more likely to continue to use a time-continuous activation than we are to jump to a new random activation, which promotes the agglomeration of longer contiguous sound grains from the corpus, even at the expense of a lower fit to the spectral template.  As such, $p_d$ a parameter that can be tuned by the artist and set closer to $1$ to promote longer grains.  We generally find $p_d \in [0.9, 0.99]$ to be effective.

    We must also specify the {\em observation probability}, which pulls the states closer to matching $\vec{v_t}$, even if they have to jump away from time continuity; otherwise, the result would sound nothing like the target. We provide details in the next section in the context of our particle filter.



\subsection{Sampling  And Synthesizing}
\label{sec:sampling}

We now describe how to apply Bayesian inference to find the sequence of activations $s_t$ and their weights $h_t$ that maximize the posterior probability given the transition model in Equation~\ref{eq:transitionprob} and the observation model, which we detail below.  The authors of \cite{wohlmayr2010probabilistic}, who use Bayesian inference based on loopy belief propagation \ChrisEdit{(Check this)} on a similar FHMM applied to multi-pitch tracking.  However, we need a faster technique for real time applications with a tunable accuracy that degrades gracefully with restricted computational resources.  To that end, we turn to a particle filter.  

Our particle filter consists of $P$ particles, each of which is a $p$-dimensional state vector (Equation~\ref{eq:statevector}) which we refer to as $\vec{s_i}$.  The particles move around the state space over time, and they each have associated weights $w_i$ that keep track of the posterior probability of that accumulated motion over all timesteps (we dispense with the time index $t$ on $\vec{s_i}$ and $w_i$ since $t$ will be clear from context).  Since each particle is its own estimate of a state that best describes what templates to choose, our goal is to sample them in such a way that (at least some of) the particles are close to representing the state representing the maximum posterior probability given all $\vec{v_t}$.

\textbf{Tracking Weights.} The particles all start out with even weights $w_i = 1/P$.  At the beginning of each time step, we sample new indices for each $\vec{s_i}$ according to Equation~\ref{eq:transitionprob}.  Then, we multiply each weight by the \textbf{observation probability}, which is based on the KL loss $d_i$ between $\Lambda_i$ and $\vec{v}_t$ (Equation~\ref{eq:approximation}) after $L$ iterations of Equation~\ref{eq:sparseklhgrad}, and that of the other particles, as follows:

\begin{equation}
    \label{eq:observationprob}
    p_O[i] = \frac{e^{-\tau d_i}}{ \sum_{j} e^{-\tau d_j}}
\end{equation}

In other words, it is a softmax based on goodness of fit to $\vec{v_t}$ with a ``temperature'' $\tau$, but using negative exponential since a larger $d_i$ loss indicates a poorer fit and hence, should indicate a lower probability.  Intuitively, a higher $\tau$ will emphasize more particles that better fit the observation, putting more importance on the observation relative than the transition probability.  This is tunable and has a similar effect to varying $p_d$ in the transition, as we will explore more in Section~\ref{sec:quantitativeevaluation}.

After multiplying each $w_i$ by $p_O[i]$, we normalize the weights so that they sum to 1.  
%If the sum of the weights is too numerically small based on the chosen floating point precision, we reset each weight to be $1/N$.

\textbf{Resampling.} The above is a naive particle filter, but it suffers from ``sample impoverishment,'' where a few particles stand out with high weights and the rest are stuck with vanishing weights, leaving the system unable to adapt to new observations.  To ameliorate this, we compute a stanard definition of the ``effective number of particles $n_{\text{eff}}$'':

\begin{equation}
    n_{\text{eff}} = \frac{1}{\sum_{i} w_i^2}
\end{equation}


which is maximized when all particles have equal weight $1/P$.  If $n_{\text{eff}}$ goes below $0.1P$ at a particular time step, we resample the particles with stochastic universal sampling (\ChrisEdit{citation}), an O(Pp) technique, and reset all of the weights to $1/N$ before continuing.  This leads to ``survival of the fittest'' where particles with a higher weight are more likely to be replicated and those with a lower weight are more likely to be eliminated.

\textbf{Synthesizing audio.} At this point, we are ready to synthesize the audio for this timestep. \ChrisEdit{Explain windowing and overlap adding}

% W is M x N, H is N x T,  we need MxT dot products, each costing N

% WH: (MxNxT), other coefficients: (NxNxT)

\textbf{Computational Complexity.} The dominant cost of both our algorithm and the algorithm of Driedger et. al. are the activation updates via KL iterations.  Given $N$ corpus templates, $T$ times, and a spectral dimension of $M$, for $L$ KL iterations, the time complexity of Driedger (Equation~\ref{eq:klhgrad}) is $O(L(N^2T + MNT))$.  On the other hand, given $P$ particles in our algorithm and $p$ activations per particle, the time complexity of our analagous Equation~\ref{eq:sparseklhgrad} is only $O(LP(p^2T + MpT))$, which does not scale with the corpus size $N$ (though $P$ may need to scale with $N$ for good results, as argued in Section~\ref{sec:activationprob}).  As an example, for 5 minute corpus a window length of 2048 ($M=1024$, hop=$1024$) at a sample rate of 44100, using $P=1000$ and $p=10$, this is a speedup of nearly 20x.  We note also that propagating particles and applying the observation model are also embarrassingly parallelizable at the particle level, and regardless, our algorithm can stream the results real time.


\subsection{Bells And Whistles (Pun Intended)}

Though the main algorithm stands on its own, we have implemented several improvements.



\textbf{Regularizing Quiet Moments in The Corpus.}

Talk about regularization and proposal distribution


\textbf{Mel Spectrograms.} The computational complexity scales linearly with $P$, so lowering $P$ makes the system run faster, but it is also possible to decrease $M$ without sacrificing much quality.  One can replace the full spectrogram wiht a mel-spaced spectrogram with many fewer bins.  In fact, Schwarz's Caterpillar system did something very similar suggesting \cite{schwarz2000system}, and we see results quite similar to the full on STFT (Section~\ref{sec:quantitativeevaluation}).


\textbf{Proposal Distribution.} We use a proposal distribution

Note that \cite{buch2017nichtnegativematrixfaktorisierungnutzendesklangsynthesensystem} did something similar




\section{Practical Considerations}

\subsection{How Many Particles?}
\label{sec:activationprob}

In practice, seemingly few particles are surprisingly effective at picking up on activations that fit the target well, which we explain with a simple probabilistic argument.  Given a corpus with $N$ sound grains (including pitch shifts) and $P$ particles that each capture $p$ activations, suppose that we have a hypothetical ``ideal particle'' $\hat{h}$ with the $p$ best activations that work together to match a target.  Since activations adapt to the target when they are randomly resampled with probability $p_d$, the probability of jumping to at least one of the top $k$ activations of $\hat{h}$ at a particular timestep is 

\begin{equation}
    \label{eq:timeadjacentprob}
    1 - \left( p_d + (1-p_d) \frac{(N-2-k)}{N} \right)^{pP}
\end{equation}

This probability is small (about 0.058, 0.077, 0.095 for $k = 1, 2, 3$ using $N=10000$ and $P=2000$).  However, the hop length is quite small (1024/44100 ~= 23 milliseconds), so if we allow the right activation to be chosen slightly before or slightly after the exact time it should be chosen, we wouldn't notice it and the result would still sound good.  Furthermore, for each activation in $\hat{h}$, there are usually several activations in the corpus that sound acceptably similar.  Let $\delta$ be the amount of wiggle room before or after in time for choosing the best activations, and let $w$ be a factor of acceptible activations nearby (e.g. $w=3$ would allow the us to consider each activation in $\hat{x}$ and its closest two in the corpus).  Equation~\ref{eq:timeadjacentprob} is then updated as:

\begin{equation}
    \label{eq:timeadjacentprobmodified}
    1 - \left( p_d + (1-p_d) \frac{(N-2-wk)}{N} \right)^{(2 \delta +1)pP}
\end{equation}

For example, for $\delta=5$ (about 116 milliseconds of deviation) and $w = 5$, the probabilities are 0.786, 0.929, 0.976 for $k=1, 2, 3$, respectively.  These probabilities all degrade when $N$ gets larger for a larger corpus, but in that case, it is likely that the acceptable $w$ is also larger.

We also note that once one of the particles catches on to a good activation, it is promoted with a high weight and gets carried on with {\em time-continuous activations}.  This is similar to how the ``patch match'' technique in computer graphics \cite{Barnes:2009:PAR, Barnes:2010:TGP} computes nearest neighbors of many nearby patches by starting with a random initializations of nearest neighbors.  Similarly, in that case, chances are that at least one of the patches will be matched to a correct neighbor, and {\em spatially adjacent} patches correct their nearest neighbors to to neighbors at the corresponding offsets \cite{Barnes:2009:PAR}.

\subsection{Diagonal Enhancement}

Talk about negative binomial, but how it's complicated because of the way probabilities are aggregated and time-continuous activations are promoted during aggregation.  Show example plot of diagonal distribution for different $p_d$, and show a plot of Driedger as a comparison

What's interesting about Driedger is even if you set p=1, there may be more than one source active at a given time.  Whereas in this technique, there is always exactly one source if p=1.  Sometimes that sounds worse than Driedger actually because it jumps back and forth between two near guesses.  So I usually find p > 1 is better.

\subsection{Pitch Shifting}



\section{Evaluation}

\subsection{Quantitative Evaluation}
\label{sec:quantitativeevaluation}
The most important thing is that we give artists musical control.

Show fit, distribution of diagonal lengths, distribution of activation changes, and repeated activation lengths for this algorithm compared to Driedger algorithm over the Tzanetakis dataset


\subsection{Qualitative Evaluation}

Talk about Ben's ``obstacle course''

% For bibtex users:
\bibliography{ISMIRtemplate}

% For non bibtex users:
%\begin{thebibliography}{citations}
% \bibitem{Author:17}
% E.~Author and B.~Authour, ``The title of the conference paper,'' in {\em Proc.
% of the Int. Society for Music Information Retrieval Conf.}, (Suzhou, China),
% pp.~111--117, 2017.
%
% \bibitem{Someone:10}
% A.~Someone, B.~Someone, and C.~Someone, ``The title of the journal paper,''
%  {\em Journal of New Music Research}, vol.~A, pp.~111--222, September 2010.
%
% \bibitem{Person:20}
% O.~Person, {\em Title of the Book}.
% \newblock Montr\'{e}al, Canada: McGill-Queen's University Press, 2021.
%
% \bibitem{Person:09}
% F.~Person and S.~Person, ``Title of a chapter this book,'' in {\em A Book
% Containing Delightful Chapters} (A.~G. Editor, ed.), pp.~58--102, Tokyo,
% Japan: The Publisher, 2009.
%
%
%\end{thebibliography}

\end{document}

