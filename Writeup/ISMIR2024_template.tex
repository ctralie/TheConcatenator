% -----------------------------------------------
% Template for ISMIR Papers
% 2023 version, based on previous ISMIR templates

% Requirements :
% * 6+n page length maximum
% * 10MB maximum file size
% * Copyright note must appear in the bottom left corner of first page
% * Clearer statement about citing own work in anonymized submission
% (see conference website for additional details)
% -----------------------------------------------

\documentclass{article}
\usepackage[T1]{fontenc} % add special characters (e.g., umlaute)
\usepackage[utf8]{inputenc} % set utf-8 as default input encoding
\usepackage{ismir,amsmath,cite,url}
\usepackage{graphicx}
\usepackage{color}


\usepackage{lineno}
\linenumbers

\graphicspath{{../Figures/}}

% Title. Please use IEEE-compliant title case when specifying the title here,
% as it has implications for the copyright notice
% ------
\title{The Concatenator: A Bayesian Approach To Real Time Concatenative Musaicing}

% Note: Please do NOT use \thanks or a \footnote in any of the author markup

% Single address
% To use with only one author or several with the same address
% ---------------
%\oneauthor
% {Names should be omitted for double-blind reviewing}
% {Affiliations should be omitted for double-blind reviewing}

% Two addresses
% --------------
\twoauthors
  {First author} {School \\ Department}
  {Second author} {Company \\ Address}

% Three addresses
% --------------\input{ISMIR2021_paper.tex}

%\threeauthors
%  {First Author} {Affiliation1 \\ {\tt author1@ismir.edu}}
%  {Second Author} {\bf Retain these fake authors in\\\bf submission to preserve the formatting}
%  {Third Author} {Affiliation3 \\ {\tt author3@ismir.edu}}

% Four or more addresses
% OR alternative format for large number of co-authors
% ------------
%\multauthor
%{First author$^1$ \hspace{1cm} Second author$^1$ \hspace{1cm} Third author$^2$} { \bfseries{Fourth author$^3$ \hspace{1cm} Fifth author$^2$ \hspace{1cm} Sixth author$^1$}\\
%  $^1$ Department of Computer Science, University , Country\\
%$^2$ International Laboratories, City, Country\\
%$^3$  Company, Address\\
%{\tt\small CorrespondenceAuthor@ismir.edu, PossibleOtherAuthor@ismir.edu}
%}

% For the author list in the Creative Common license, please enter author names. 
% Please abbreviate the first names of authors and add 'and' between the second to last and last authors.
\def\authorname{F. Author, S. Author}

% Optional: To use hyperref, uncomment the following.
\usepackage[bookmarks=false,pdfauthor={\authorname},pdfsubject={\papersubject},hidelinks]{hyperref}
% Mind the bookmarks=false option; bookmarks are incompatible with ismir.sty.

\sloppy % please retain sloppy command for improved formatting

\begin{document}

%
\maketitle
%

\begin{abstract}

\end{abstract}

\section{Introduction}

Speed/accessibility will impact speed of research and innovation

\section{Background}


\subsection{Driedger's Original Technique}

Describe in detail

\subsection{Related Techniques}

Need some references on granular synthesis.

The idea of spectrogram decomposition used for concatenative musaicing goes back to the work of \cite{burred2013cross}.

\cite{buch2017nichtnegativematrixfaktorisierungnutzendesklangsynthesensystem} provide some improvements to the ``Let It Bee'' technique, including sidestepping phase retrieval, which we exploit in our work.

One issue with the ``Let It Bee'' technique is the sources have to be augmented with pitch shifts to hit additional pitches in the target, increasing memory consumption and runtime.  \cite{foroughmand2017multi, aarabi2018music} sidestep this by using 2D convolutive NMF \cite{schmidt2006nonnegative}, which models pitch shifts in the activations instead of the templates, saving memory.  The other added activation dimension models time history and time shifts to avoid the need for diagonal enhancement.  Interestingly, the authors apply 2D NMF to both the source material and the target, so they do not preserve the original sound grains.  However, for our preferred style, we want to take the source grains exactly as they are.

\section{Real Time Let It Bee}

\subsection{Bayesian Formulation}

    Given:
    \begin{enumerate}
        \item
            An $M \times N$ matrix $W$ of templates
        
        \item
            An $M \times 1$ observation $y_t$ at time $t$, over $T$ times
        
        \item
            A parameter $p$ for the sparsity, and a $p$-dimensional hidden state variable $x$

                \[ x_k \in [0, 1, ..., N-1], k = 0, 1, ..., p-1 \]
        
        \item
            A parameter $p_d \in [0, 1]$ for diagonal promotion
        
        \item
            Parameters $\sigma$ and $L$ for standard deviation and NMF iterations in the observation function, respectively
        
    \end{enumerate}

        Then define the {\em state transition probability} as 

		\begin{equation}
        	p(\vec{x_i} = \vec{b} | \vec{x_{i-1}} = \vec{a}) = \prod_{k=0}^{p-1} \left\{  \begin{array}{cc}  p_d & b_k = a_k+1  \\ \frac{1-p_d}{N-2} & b_k \neq a_k \\ 0 & b_k = a_k \end{array} \right\}
		\end{equation}

    
        The third case is to prevent repeated activations, but this is only enforced on adjacent frames due to the Markov property

		TODO: Define the observation probability


\subsection{Sampling Algorithm}

Discuss particle filter and synthesis details


\section{Mathematical Analysis}

\subsection{Picking Good Activations}

In practice, a limited number of particles are extremely effective at picking up on activations that fit the target well.  To see why, we can invoke a simple probabilistic argument.  Given a corpus with $N$ sound grains (including pitch shifts) and $P$ particles that capture $p$ activations, suppose that the initial particles are chosen uniformly at random over the set of combinations $\binom{N}{p}$.  Suppose also that we have a hypothetical ``ideal particle'' $\hat{x}$ with the $p$ best activations that work together to match a target.  Then, the probability that at least one particle captures at least one of the top $k$ activations in $\hat{x}$ is  

\begin{equation}
    \label{eq:initialsampleprob}
    1 - \left(\binom{N-k}{p} / \binom{N}{p}\right)^P = 1 - \left( \frac{(N-p)...(N-p-k+1)}{N..(N-k+1)}\right) ^P
\end{equation}

As an example, if $N = 10000$ (~4 minutes of corpus audio for hop=1024, sr=44100), $P = 2000$, and $p=10$, the probabilities are 0.86, 0.98, and 0.998 for $k=1, 2, 3$, respectively.  This is similar to how the ``patch match'' technique in computer graphics \cite{Barnes:2009:PAR, Barnes:2010:TGP} computes nearest neighbors of many nearby patches by starting with a random initializations of nearest neighbors.  Similarly, in that case, chances are that at least one of the patches will be matched to a correct neighbor, and spatially adjacent patches can have their nearest neighbors corrected to neighbors at the corresponding offsets \cite{Barnes:2009:PAR}.  This is analagous to our propagation to {\em time-adjacent} activations in subsequent timesteps.

The above analysis is promising, but such an initialization only happens right at the beginning, and the best activation at any given time transitions as the sound transitions.  The more common way for the activations to adapt to the target is when they are randomly resampled with probability $p_d$.  Under this sampling, the probability of jumping to at least one of the top $k$ activations of $\hat{x}$ at a particular timestep is 

\begin{equation}
    \label{eq:timeadjacentprob}
    1 - \left( p_d + (1-p_d) \frac{(N-2-k)}{N} \right)^{pP}
\end{equation}

This probability is much smaller (about 0.058, 0.077, 0.095 for $k = 1, 2, 3$ using $N=10000$ and $P=2000$).  However, the hop length is quite small (1024/44100 ~= 23 milliseconds), so if we allow the right activation to be chosen slightly before or slightly after the exact time it should be chosen, we wouldn't notice it and the result would still sound good.  Furthermore, for each activation in $\hat{x}$, there are usually several activations in the corpus that sound acceptably similar.  Let $\delta$ be the amount of wiggle room before or after in time for choosing the best activations, and let $w$ be a factor of acceptible activations nearby (e.g. $w=3$ would allow the us to consider each activation in $\hat{x}$ and its closest two in the corpus).  The Equation~\ref{eq:timeadjacentprob} can be modified to:

\begin{equation}
    \label{eq:timeadjacentprobmodified}
    1 - \left( p_d + (1-p_d) \frac{(N-2-wk)}{N} \right)^{(2 \delta +1)pP}
\end{equation}

For example, for $\delta=5$ (about 116 milliseconds of deviation) and $w = 5$, the probabilities are 0.786, 0.929, 0.976 for $k=1, 2, 3$, respectively.  These probabilities all degrade when $N$ gets larger for a larger corpus, but in that case, it is likely that the acceptable $w$ is also larger.


TODO: Make a table for a few different choices of $N$ and $P$ for comparison


\subsection{Diagonal Enhancement}

Talk about negative binomial, show example plot of diagonal distribution for different $p_d$


\section{Evaluation}

\subsection{Quantitative Evaluation}
Show fit, distribution of diagonal lengths, distribution of activation changes, and repeated activation lengths for this algorithm compared to Driedger algorithm over the Tzanetakis dataset


\subsection{Qualitative Evaluation}

Talk about Ben's ``obstacle course''

% For bibtex users:
\bibliography{ISMIRtemplate}

% For non bibtex users:
%\begin{thebibliography}{citations}
% \bibitem{Author:17}
% E.~Author and B.~Authour, ``The title of the conference paper,'' in {\em Proc.
% of the Int. Society for Music Information Retrieval Conf.}, (Suzhou, China),
% pp.~111--117, 2017.
%
% \bibitem{Someone:10}
% A.~Someone, B.~Someone, and C.~Someone, ``The title of the journal paper,''
%  {\em Journal of New Music Research}, vol.~A, pp.~111--222, September 2010.
%
% \bibitem{Person:20}
% O.~Person, {\em Title of the Book}.
% \newblock Montr\'{e}al, Canada: McGill-Queen's University Press, 2021.
%
% \bibitem{Person:09}
% F.~Person and S.~Person, ``Title of a chapter this book,'' in {\em A Book
% Containing Delightful Chapters} (A.~G. Editor, ed.), pp.~58--102, Tokyo,
% Japan: The Publisher, 2009.
%
%
%\end{thebibliography}

\end{document}

