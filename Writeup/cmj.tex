 %
%-----------------------------------------------------------
%% Computer Music Journal LaTeX template
%%
%% September  2009
%% Author: Cornelia Kreutzer, University of Limerick



%---Document preamble
%
\documentclass[letterpaper, 12pt]{article}


\usepackage{cmjStyle} %use CMJ style
\usepackage{natbib} %natbib package, necessary for customized cmj BibTeX style
\bibpunct{(}{)}{;}{a}{}{, } %adapt style of references in text
\doublespacing
\raggedright % use this to remove spacing and hyphenation oddities
\setlength{\parskip}{2ex}
\parindent 24pt
\urlstyle{same} % make url tags have the same font
\setcounter{secnumdepth}{-1} % remove section numbering
\usepackage{epstopdf}
\usepackage{amsmath,amssymb,amsbsy,bm,upgreek,nicefrac}
\usepackage{todonotes,microtype}

% Use the Figures subfolder for image files
\graphicspath{{../Figures/}}


%% ----------------------------------------------------------------------------------------------------------------------------------------
%% CMJ page headers
%% For initial submission use \lhead{Anonymous}
%% On acceptance for publication, use real author surnames for \lhead modeled on the following examples
%%		One author:	\lhead{\small Keislar}
%%		Two authors:	\lhead{\small Keislar and Castine}
%%		Three authors:	\lhead{\small Keislar, Castine, and Rundall}
%%		Four or more:	\lhead{\small Keislar et al.}
%%
\lhead{\small Tralie and Cantil}


%% The package endfloat moves all floats (figures, tables...) to the end of the article, as required for the final version of a CMJ article.
%% Leave this package commented out for initial submission, but uncomment it and the following callout commands for the final version. 
% \usepackage{endfloat}
% \renewcommand{\figureplace}{%
%	\begin{center}
%		\textbf{<<TYPE: INSERT \figurename~\thepostfig\ ABOUT HERE.>>}
%	\end{center}}
% \renewcommand{\tableplace}{%
%	\begin{center}
%		\textbf{<<TYPE: INSERT \tablename~\theposttbl\ ABOUT HERE.>>}
%	\end{center}}

%---Document----------
\begin{document}

{\cmjTitle Let It Bee Realtime: A Bayesian Approach To Concatenative Musaicing}
\vspace*{24pt}

%(In the initial submission, omit all the following author information to ensure anonymity during peer review. On final submission please make sure that the author address is a complete, functioning postal address. Post will be sent to that address.)

% Author: name
{\cmjAuthor Christopher J. Tralie, Ben Cantil}	% List all authors here
							% e.g.:
							% {\cmjAuthor Doug Keislar, Peter Castine, and Jake Rundall}
 
% Author: address
\begin{cmjAuthorAddress}
	Department of Mathematics And Computer Science\\
	Ursinus College\\
	601 East Main Street\\
	Collegeville, PA 19426 USA\\		% Adapt as needed for non-US addresses
	ctralie@alumni.princeton.edu
\end{cmjAuthorAddress}


\begin{abstract}

\end{abstract}

\section{Introduction}

Speed/accessibility will impact speed of research and innovation

\section{Background}


\subsection{Driedger's Original Technique}

Describe in detail

\subsection{Related Techniques}

Need some references on granular synthesis.

The idea of spectrogram decomposition used for concatenative musaicing goes back to the work of \cite{burred2013cross}.

\cite{buch2017nichtnegativematrixfaktorisierungnutzendesklangsynthesensystem} provide some improvements to the ``Let It Bee'' technique, including sidestepping phase retrieval, which we exploit in our work.

One issue with the ``Let It Bee'' technique is the sources have to be augmented with pitch shifts to hit additional pitches in the target, increasing memory consumption and runtime.  \cite{foroughmand2017multi, aarabi2018music} sidestep this by using 2D convolutive NMF \cite{schmidt2006nonnegative}, which models pitch shifts in the activations instead of the templates, saving memory.  The other added activation dimension models time history and time shifts to avoid the need for diagonal enhancement.  Interestingly, the authors apply 2D NMF to both the source material and the target, so they do not preserve the original sound grains.  However, for our preferred style, we want to take the source grains exactly as they are.

\section{Real Time Let It Bee}

\subsection{Bayesian Formulation}

    Given:
    \begin{enumerate}
        \item
            An $M \times N$ matrix $W$ of templates
        
        \item
            An $M \times 1$ observation $y_t$ at time $t$, over $T$ times
        
        \item
            A parameter $p$ for the sparsity, and a $p$-dimensional hidden state variable $x$

                \[ x_k \in [0, 1, ..., N-1], k = 0, 1, ..., p-1 \]
        
        \item
            A parameter $p_d \in [0, 1]$ for diagonal promotion
        
        \item
            Parameters $\sigma$ and $L$ for standard deviation and NMF iterations in the observation function, respectively
        
    \end{enumerate}

        Then define the {\em state transition probability} as 

		\begin{equation}
        	p(\vec{x_i} = \vec{b} | \vec{x_{i-1}} = \vec{a}) = \prod_{k=0}^{p-1} \left\{  \begin{array}{cc}  p_d & b_k = a_k+1  \\ \frac{1-p_d}{N-2} & b_k \neq a_k \\ 0 & b_k = a_k \end{array} \right\}
		\end{equation}

    
        The third case is to prevent repeated activations, but this is only enforced on adjacent frames due to the Markov property

		TODO: Define the observation probability


\subsection{Sampling Algorithm}

Discuss particle filter and synthesis details


\section{Mathematical Analysis}

\subsection{Picking Good Activations}

In practice, a limited number of particles are extremely effective at picking up on activations that fit the target well.  To see why, we can invoke a simple probabilistic argument.  Given a corpus with $N$ sound grains (including pitch shifts) and $P$ particles that capture $p$ activations, suppose that the initial particles are chosen uniformly at random over the set of combinations $\binom{N}{p}$.  Suppose also that we have a hypothetical ``ideal particle'' $\hat{x}$ with the $p$ best activations that work together to match a target.  Then, the probability that at least one particle captures at least one of the top $k$ activations in $\hat{x}$ is  

\begin{equation}
    \label{eq:initialsampleprob}
    1 - \left(\binom{N-k}{p} / \binom{N}{p}\right)^P = 1 - \left( \frac{(N-p)...(N-p-k+1)}{N..(N-k+1)}\right) ^P
\end{equation}

As an example, if $N = 10000$ (~4 minutes of corpus audio for hop=1024, sr=44100), $P = 2000$, and $p=10$, the probabilities are 0.86, 0.98, and 0.998 for $k=1, 2, 3$, respectively.  This is similar to how the ``patch match'' technique in computer graphics \cite{Barnes:2009:PAR, Barnes:2010:TGP} computes nearest neighbors of many nearby patches by starting with a random initializations of nearest neighbors.  Similarly, in that case, chances are that at least one of the patches will be matched to a correct neighbor, and spatially adjacent patches can have their nearest neighbors corrected to neighbors at the corresponding offsets \cite{Barnes:2009:PAR}.  This is analagous to our propagation to {\em time-adjacent} activations in subsequent timesteps.

The above analysis is promising, but such an initialization only happens right at the beginning, and the best activation at any given time transitions as the sound transitions.  The more common way for the activations to adapt to the target is when they are randomly resampled with probability $p_d$.  Under this sampling, the probability of jumping to at least one of the top $k$ activations of $\hat{x}$ at a particular timestep is 

\begin{equation}
    \label{eq:timeadjacentprob}
    1 - \left( p_d + (1-p_d) \frac{(N-2-k)}{N} \right)^{pP}
\end{equation}

This probability is much smaller (about 0.058, 0.077, 0.095 for $k = 1, 2, 3$ using $N=10000$ and $P=2000$).  However, the hop length is quite small (1024/44100 ~= 23 milliseconds), so if we allow the right activation to be chosen slightly before or slightly after the exact time it should be chosen, we wouldn't notice it and the result would still sound good.  Furthermore, for each activation in $\hat{x}$, there are usually several activations in the corpus that sound acceptably similar.  Let $\delta$ be the amount of wiggle room before or after in time for choosing the best activations, and let $w$ be a factor of acceptible activations nearby (e.g. $w=3$ would allow the us to consider each activation in $\hat{x}$ and its closest two in the corpus).  The Equation~\ref{eq:timeadjacentprob} can be modified to:

\begin{equation}
    \label{eq:timeadjacentprobmodified}
    1 - \left( p_d + (1-p_d) \frac{(N-2-wk)}{N} \right)^{(2 \delta +1)pP}
\end{equation}

For example, for $\delta=5$ (about 116 milliseconds of deviation) and $w = 5$, the probabilities are 0.786, 0.929, 0.976 for $k=1, 2, 3$, respectively.  These probabilities all degrade when $N$ gets larger for a larger corpus, but in that case, it is likely that the acceptable $w$ is also larger.


TODO: Make a table for a few different choices of $N$ and $P$ for comparison


\subsection{Diagonal Enhancement}

Talk about negative binomial, show example plot of diagonal distribution for different $p_d$


\section{Evaluation}

\subsection{Quantitative Evaluation}
Show fit, distribution of diagonal lengths, distribution of activation changes, and repeated activation lengths for this algorithm compared to Driedger algorithm over the Tzanetakis dataset


\subsection{Qualitative Evaluation}

Talk about Ben's ``obstacle course''


%References
\bibliographystyle{cmj}
\bibliography{cmjbib}

\end{document}
